{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a91acac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTTT(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1296, out_features=128, bias=True)\n",
      "  (pi): Linear(in_features=128, out_features=81, bias=True)\n",
      "  (v): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from multiprocessing import Pool\n",
    "import concurrent.futures\n",
    "\n",
    "from utils import State, Action, is_terminal, change_state, get_all_valid_actions, terminal_utility, is_valid_action, invert\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#we can stack arbitrary number of Residualblock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels, eps=1e-5, momentum=0.09)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels, eps=1e-5, momentum=0.09)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + residual)  # Skip connection\n",
    "\n",
    "#if using batchnorm, bias=False, can add additional FC/conv layer also\n",
    "class UTTT(nn.Module):\n",
    "    def __init__(self, residual=True, batchnorm=True):\n",
    "        super(UTTT, self).__init__()\n",
    "        \n",
    "        self.channels = 8\n",
    "        self.residual = residual\n",
    "        self.fc_size1 = 128\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, self.channels, kernel_size=3, stride=1, padding=1, bias= not self.batchnorm)\n",
    "        self.bn1 = nn.BatchNorm2d(self.channels, eps=1e-5, momentum=0.09) if self.batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(self.channels, self.channels*2, kernel_size=3, stride=1, padding=1, bias= not self.batchnorm)\n",
    "        self.bn2 = nn.BatchNorm2d(self.channels, eps=1e-5, momentum=0.09) if self.batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, bias= not self.batchnorm)\n",
    "        self.bn3 = nn.BatchNorm2d(self.channels, eps=1e-5, momentum=0.09) if self.batchnorm else None\n",
    "        \n",
    "        self.res2 = ResidualBlock(self.channels) if self.residual else None\n",
    "        self.res3 = ResidualBlock(self.channels) if self.residual else None\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.channels*2 * 9 * 9, self.fc_size1)  # Input size: 128 filters × 9 × 9\n",
    "\n",
    "        # Output layers\n",
    "        self.pi = nn.Linear(self.fc_size1, 81)  # Policy output (Softmax for move probabilities)\n",
    "        self.v = nn.Linear(self.fc_size1, 1)    # Value output (Tanh for game state evaluation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))) if self.batchnorm else F.relu(self.conv1(x))\n",
    "\n",
    "        # Residual connections\n",
    "        if self.residual:\n",
    "            x = self.res2(x)\n",
    "            x = self.res3(x)\n",
    "        else:\n",
    "            x = F.relu(self.bn2(self.conv2(x))) if self.batchnorm else F.relu(self.conv2(x))\n",
    "            x = F.relu(self.bn3(self.conv3(x))) if self.batchnorm else F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and pass through FC layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Outputs\n",
    "        pi = F.softmax(self.pi(x), dim=1)  # Policy head (logits, apply softmax in loss function)\n",
    "        v = F.tanh(self.v(x))  # Value head (range [-1, 1])\n",
    "\n",
    "        return pi, v\n",
    "\n",
    "class _UTTT(nn.Module):\n",
    "    def __init__(self, residual=True, batchnorm=True):\n",
    "        super(_UTTT, self).__init__()\n",
    "        \n",
    "        self.channels = 8\n",
    "        self.residual = residual\n",
    "        self.fc_size1 = 128\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, self.channels, kernel_size=3, stride=1, padding=1, bias= not self.batchnorm)\n",
    "        self.bn1 = nn.BatchNorm2d(self.channels, eps=1e-5, momentum=0.09) if self.batchnorm else None\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(self.channels, self.channels*2, kernel_size=3, stride=1, padding=1, bias= not self.batchnorm)\n",
    "        self.bn2 = nn.BatchNorm2d(self.channels, eps=1e-5, momentum=0.09) if self.batchnorm else None\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.channels*2, self.channels*2, kernel_size=3, stride=1, padding=1, bias= not self.batchnorm)\n",
    "        self.bn3 = nn.BatchNorm2d(self.channels, eps=1e-5, momentum=0.09) if self.batchnorm else None\n",
    "        \n",
    "        self.res2 = ResidualBlock(self.channels) if self.residual else None\n",
    "        self.res3 = ResidualBlock(self.channels) if self.residual else None\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.channels*2 * 9 * 9, self.fc_size1)  # Input size: 128 filters × 9 × 9\n",
    "\n",
    "        # Output layers\n",
    "        self.v = nn.Linear(self.fc_size1, 1)    # Value output (Tanh for game state evaluation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))) if self.batchnorm else F.relu(self.conv1(x))\n",
    "\n",
    "        # Residual connections\n",
    "        if self.residual:\n",
    "            x = self.res2(x)\n",
    "            x = self.res3(x)\n",
    "        else:\n",
    "            x = F.relu(self.bn2(self.conv2(x))) if self.batchnorm else F.relu(self.conv2(x))\n",
    "            x = F.relu(self.bn3(self.conv3(x))) if self.batchnorm else F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and pass through FC layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Outputs\n",
    "        v = F.tanh(self.v(x))  # Value head (range [-1, 1])\n",
    "\n",
    "        return v\n",
    "    \n",
    "# Instantiate the model\n",
    "model = UTTT(False,False)\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "def get_optimizer(model, lr=0.0001):\n",
    "    return optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define loss functions\n",
    "def compute_policy_loss(predicted_policy, true_policy):\n",
    "    #return nn.CrossEntropyLoss()(predicted_policy, true_policy)\n",
    "    policy_loss_fn = torch.nn.CrossEntropyLoss()  # For policy (classification)\n",
    "    return policy_loss_fn(predicted_policy, true_policy)\n",
    "\n",
    "def compute_value_loss(predicted_value, true_value):\n",
    "    #return nn.MSELoss()(predicted_value, true_value)\n",
    "    value_loss_fn = torch.nn.MSELoss()  # For value (regression)\n",
    "    return value_loss_fn(predicted_value, true_value)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_array(state):\n",
    "    \"\"\"\n",
    "    Convert the Ultimate Tic-Tac-Toe board state into a 9x9 array, making sure it's hashable.\n",
    "    \n",
    "    Args:\n",
    "        state: The current game state (State object).\n",
    "    \n",
    "    Returns:\n",
    "        A NumPy array (9, 9) representation of the board (with current player layer).\n",
    "    \"\"\"\n",
    "    player = 1 if state.fill_num==1 else -1\n",
    "    \n",
    "    # Get the board from the state\n",
    "    board = state.board  # This is a 3x3x3x3 ndarray\n",
    "\n",
    "    # Flatten the 3x3x3x3 board into a 9x9 representation\n",
    "    board_array = np.zeros((9, 9), dtype=np.float32)\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    board_array[i * 3 + k][j * 3 + l] = board[i][j][k][l]\n",
    "\n",
    "    # Normalize the board values with flip\n",
    "    board_array[board_array == 1] = 1.0 * player\n",
    "    board_array[board_array == 2] = -1.0 * player\n",
    "    board_array[board_array == 0] = 0.0\n",
    "\n",
    "    return np.array(board_array)\n",
    "\n",
    "def action_to_index(action):\n",
    "    \"\"\"\n",
    "    Convert the action tuple (meta_row, meta_col, local_row, local_col) into an integer index for the 9x9 grid.\n",
    "\n",
    "    Args:\n",
    "        action: Tuple (meta_row, meta_col, local_row, local_col) representing the coordinates on the boards.\n",
    "    \n",
    "    Returns:\n",
    "        Integer index corresponding to the 9x9 grid.\n",
    "    \"\"\"\n",
    "    meta_row, meta_col, local_row, local_col = action\n",
    "    \n",
    "    # Map meta-board (meta_row, meta_col) to a flat index in the range [0, 8]\n",
    "    meta_index = meta_row * 3 + meta_col  # 3x3 meta-board\n",
    "    \n",
    "    # Map local-board (local_row, local_col) to a flat index in the range [0, 8]\n",
    "    local_index = local_row * 3 + local_col  # 3x3 local-board\n",
    "    \n",
    "    # Final index in the flattened 9x9 grid\n",
    "    return meta_index * 9 + local_index  # 9x9 flattened grid\n",
    "\n",
    "def index_to_action(index: int) -> Action:\n",
    "    \"\"\"\n",
    "    Convert an action index back to the action tuple (meta_row, meta_col, local_row, local_col).\n",
    "    \n",
    "    Args:\n",
    "        index (int): The action index.\n",
    "        \n",
    "    Returns:\n",
    "        Action: The corresponding action tuple (meta_row, meta_col, local_row, local_col).\n",
    "    \"\"\"\n",
    "    # Calculate the meta-row and meta-column from the index\n",
    "    meta_row = index // 27\n",
    "    meta_col = (index % 27) // 9\n",
    "    \n",
    "    # Calculate the local row and local column from the index\n",
    "    local_row = (index % 9) // 3\n",
    "    local_col = index % 3\n",
    "    \n",
    "    return (meta_row, meta_col, local_row, local_col)\n",
    "\n",
    "def normalize_policy(policy):\n",
    "    \"\"\"\n",
    "    Normalize the policy and ensure no NaN or infinite values.\n",
    "    \n",
    "    Args:\n",
    "        policy: A numpy array containing the policy probabilities for each action.\n",
    "        valid_mask: A mask array where valid actions are 1, invalid actions are 0.\n",
    "    \n",
    "    Returns:\n",
    "        A normalized policy.\n",
    "    \"\"\"    \n",
    "    # Check if any valid actions are left\n",
    "    if np.sum(policy) > 0:\n",
    "        policy = policy / np.sum(policy)  # Normalize\n",
    "    else:\n",
    "        # If no valid actions, reset to uniform distribution over valid actions\n",
    "        policy = np.ones_like(policy) / len(policy)\n",
    "    \n",
    "    # Check for NaN or infinite values in the policy and reset if needed\n",
    "    if np.any(np.isnan(policy)) or np.any(np.isinf(policy)):\n",
    "        policy = np.ones_like(policy) / len(policy)  # Reset to uniform distribution\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def state_to_tensor(state):\n",
    "    return torch.tensor(board_to_array(state), dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(device)  # Add batch and channel dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58665e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    __slots__ = ('state', 'children', 'N', 'W', 'Q', 'P')\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        #self.parent = parent\n",
    "        #self.action = action\n",
    "        #self.action_probability = None\n",
    "        self.children = []\n",
    "        \n",
    "        self.N = 0\n",
    "        #self.visit_count = 0\n",
    "        \n",
    "        self.W = 0\n",
    "        #self.total_reward = 0 self.state_value_sum = 0.0\n",
    "        \n",
    "        self.Q = 0\n",
    "        #self.q_value = 0 self.state_value_mean = 0.0\n",
    "        \n",
    "        self.P = None\n",
    "        #self.prior_policy = None self.state_value = None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return is_terminal(self.state)\n",
    "    \n",
    "    def no_valid_actions(self):\n",
    "        return len(get_all_valid_actions(self.state)) == 0\n",
    "    \n",
    "    def best_child(self, cpuct): #cpuct=exploration_weight\n",
    "        #return max(self.children, key=lambda child: child.W / child.N + cpuct * child.P * math.sqrt(math.log(self.N) / (1 + child.N)))\n",
    "        #return max(self.children, key=lambda child: child.Q + cpuct * child.P * math.sqrt(self.N) / (1 + child.N))\n",
    "        #values = np.array([child.W / child.N + cpuct * child.P * np.sqrt(np.log(self.N) / (1 + child.N)) for child in self.children])\n",
    "        #values = np.array([child.Q + cpuct * child.P * np.sqrt(self.N) / (1 + child.N) for child in self.children])\n",
    "        values = np.array([-child.Q + cpuct * child.P * np.sqrt(self.N) / (1 + child.N) for child in self.children])\n",
    "        best_idx = np.argmax(values)\n",
    "        return self.children[best_idx], best_idx\n",
    "    \n",
    "    def best_action(self, cpuct):\n",
    "        return get_all_valid_actions(self.state)[self.best_child(cpuct)[1]]\n",
    "\n",
    "    \n",
    "class MCTS:\n",
    "    __slots__ = ('model', 'cpuct', 'tree')\n",
    "    def __init__(self, model, cpuct=2.0):\n",
    "        self.model = model\n",
    "        self.cpuct = cpuct\n",
    "        self.tree = {}\n",
    "    \n",
    "    def search(self, root_state, simulations=200):\n",
    "        root = self.get_or_create_node(root_state)\n",
    "        \n",
    "        if root.is_terminal() or root.no_valid_actions():\n",
    "            return\n",
    "        \n",
    "        for _ in range(simulations):\n",
    "            node = root\n",
    "            path = [node]\n",
    "            \n",
    "            # Selection\n",
    "            while not node.is_leaf():\n",
    "                node = node.best_child(self.cpuct)[0]\n",
    "                path.append(node)\n",
    "            \n",
    "            pi, v = self.model(self.get_state_tensor(node.state))\n",
    "            v = v.item()\n",
    "            \n",
    "            # Expansion\n",
    "            if not node.is_terminal() and not node.no_valid_actions():\n",
    "                valid_actions = get_all_valid_actions(node.state)\n",
    "                self.expand_node(node, valid_actions, pi)\n",
    "            \n",
    "            # Evaluation\n",
    "            v = self.evaluate(node, v)\n",
    "            \n",
    "            # Backpropagation\n",
    "            for node in reversed(path):\n",
    "                node.N += 1\n",
    "                node.W += v\n",
    "                node.Q = node.W / node.N\n",
    "                v *= -1\n",
    "            #print(path)\n",
    "        return root.best_action(self.cpuct)\n",
    "    \n",
    "    def expand_node(self, node, valid_actions, pi):\n",
    "        valid_mask = np.zeros(81)\n",
    "        action_idxs = [action_to_index(a) for a in valid_actions]\n",
    "        valid_mask[action_idxs] = 1\n",
    "        pi = pi.detach().cpu().numpy().reshape(81) * valid_mask  # Convert to numpy and apply valid mask\n",
    "        pi = pi / np.sum(pi)  # Normalize the policy to sum to 1\n",
    "        for action, action_idx in zip(valid_actions, action_idxs):\n",
    "            child_state = change_state(node.state, action)\n",
    "            child_node = Node(child_state)\n",
    "            child_node.P = pi[action_idx]\n",
    "            node.children.append(child_node)\n",
    "\n",
    "    def evaluate(self, node, v):\n",
    "        if is_terminal(node.state):\n",
    "            #return 2*terminal_utility(node.state)-1\n",
    "            #return 0 if 2*terminal_utility(node.state)-1==0 else 1\n",
    "            return 0 if 2*terminal_utility(node.state)-1==0 else -1\n",
    "            #remember v is utility to current player. so terminal state, current player to move loses so -1\n",
    "            #otherwise, model predicts v which is utiilty for current player\n",
    "            #then we need to use -child.Q since we are maximizing (from POV of current player)\n",
    "            #i.e. current player wants to find most -ve child.Q since that is worst for child -> best for current\n",
    "        return v\n",
    "    \n",
    "    def get_or_create_node(self, state):\n",
    "        state_tuple = tuple(map(tuple, board_to_array(state)))\n",
    "        if state_tuple not in self.tree:\n",
    "            self.tree[state_tuple] = Node(state)\n",
    "        return self.tree[state_tuple]\n",
    "    \n",
    "    #@lru_cache(maxsize=10000)\n",
    "    def get_state_tensor(self, state):\n",
    "        return torch.tensor(board_to_array(state), dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "train_episodes = 100\n",
    "mcts_search = 200 #100 #400 #600\n",
    "n_pit_network = 50 #20\n",
    "threshold = 0.52 #0.50 #0.55\n",
    "temperature = 0.05 #lower is more deterministic\n",
    "playgames_before_training = 2 #4 #5 #25\n",
    "parallel_games = 8\n",
    "cpuct = 2\n",
    "training_epochs = 4\n",
    "learning_rate = 0.0001\n",
    "save_model_path = 'training'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ffe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_probs(state, mcts, simulations=200):\n",
    "    \"\"\"\n",
    "    Get the action probabilities from the MCTS search for a given board state.\n",
    "    \n",
    "    Args:\n",
    "        state: The initial game state (State object).\n",
    "        mcts: The MCTS instance used for search.\n",
    "        simulations: Number of MCTS simulations to run.\n",
    "    \n",
    "    Returns:\n",
    "        action_probs: A numpy array of size 81 representing the action probabilities.\n",
    "    \"\"\"\n",
    "    # Perform MCTS search\n",
    "    #best_action = mcts.search(state, simulations)\n",
    "    mcts.search(state, simulations)\n",
    "    \n",
    "    print(\"Done one iteration of MCTS\")\n",
    "    \n",
    "    # Initialize action probabilities\n",
    "    action_probs = np.zeros(81)\n",
    "    \n",
    "    # Get visit counts for valid actions\n",
    "    root_node = mcts.get_or_create_node(state)\n",
    "    #child_Ns = {child.action: child.N for child in root_node.children}\n",
    "    child_Ns = {get_all_valid_actions(root_node.state)[idx]: child.N for idx, child in enumerate(root_node.children)}\n",
    "    \n",
    "    if root_node.N > 0:\n",
    "        for action, N in child_Ns.items():\n",
    "            action_probs[action_to_index(action)] = N / root_node.N\n",
    "    #print(state, action_probs)\n",
    "    return action_probs\n",
    "\n",
    "def playgame(mcts, simulations=200):\n",
    "    \"\"\"\n",
    "    Simulate one game of Ultimate Tic-Tac-Toe, utilizing MCTS for decision-making.\n",
    "    \n",
    "    Args:\n",
    "        mcts: The MCTS instance used for decision-making.\n",
    "        simulations: Number of MCTS simulations per move.\n",
    "    \n",
    "    Returns:\n",
    "        game_mem: A list of game memory (state, player, action probability, game result).\n",
    "    \"\"\"\n",
    "    game_mem = []\n",
    "    state = State()\n",
    "    \n",
    "    while True:\n",
    "        #print(state)\n",
    "        # Check if the game is over\n",
    "        if len(get_all_valid_actions(state)) == 0 or is_terminal(state):\n",
    "            #need to include terminal state? but what policy to put?\n",
    "            #remember model predicts utility for current player, so\n",
    "            #if result==1 (p1 win) and current player==1 then 1*1=1\n",
    "            #if result==1 (p1 win) and current player==2 then 1*-1=-1\n",
    "            #if result==-1 (p2 win) and current player==1 then -1*1=-1\n",
    "            #if result==-1 (p2 win) and current player==2 then -1*-1=1\n",
    "            result = 2*terminal_utility(state)-1\n",
    "            for mem in game_mem:\n",
    "                mem[3] = result * (1 if mem[1]==1 else -1)\n",
    "            return game_mem\n",
    "\n",
    "        # Get action probabilities using MCTS\n",
    "        policy = get_action_probs(state, mcts, simulations)\n",
    "        policy = policy / np.sum(policy)  # Normalize the policy\n",
    "        #game_mem.append([board_to_array(state), state.fill_num, policy, None])\n",
    "        game_mem.append([state, state.fill_num, policy, None])\n",
    "        \n",
    "        # Choose an action based on the policy\n",
    "        action_index = np.random.choice(len(policy), p=policy)\n",
    "        action = index_to_action(action_index)\n",
    "        \n",
    "        #print(\"Policy:\", policy)\n",
    "        #print(\"Chosen action:\", action)\n",
    "        \n",
    "        state = change_state(state, action)\n",
    "\n",
    "def playgames(mctss, num_games=8, simulations=200):\n",
    "    \"\"\"\n",
    "    Run multiple games in parallel using batched MCTS.\n",
    "    \n",
    "    Args:\n",
    "        mctss: List of MCTS instances\n",
    "        num_games: Number of games to run in parallel.\n",
    "        simulations: Number of MCTS simulations per move.\n",
    "    \n",
    "    Returns:\n",
    "        games_mem: A list containing `num_games` game memory logs.\n",
    "    \"\"\"\n",
    "    games_mem = [[] for _ in range(num_games)]\n",
    "    states = [State() for _ in range(num_games)]\n",
    "    active_games = np.ones(num_games, dtype=bool)  # Track which games are still running\n",
    "\n",
    "    while active_games.any():\n",
    "        # Apply actions and check game termination\n",
    "        for i in range(num_games):\n",
    "            if active_games[i]:\n",
    "                if len(get_all_valid_actions(states[i])) == 0 or is_terminal(states[i]):\n",
    "                    #remember model predicts utility for current player, so\n",
    "                    #if result==1 (p1 win) and current player==1 then 1*1=1\n",
    "                    #if result==1 (p1 win) and current player==2 then 1*-1=-1\n",
    "                    #if result==-1 (p2 win) and current player==1 then -1*1=-1\n",
    "                    #if result==-1 (p2 win) and current player==2 then -1*-1=1\n",
    "                    result = 2*terminal_utility(states[i])-1\n",
    "                    for mem in games_mem[i]:\n",
    "                        mem[3] = result * (1 if mem[1]==1 else -1)\n",
    "                    active_games[i] = False  # Mark game as finished\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_games) as executor:\n",
    "            results = executor.map(lambda mcts,state: get_action_probs(state,mcts,simulations), mctss, states)\n",
    "        \n",
    "        policies = list(results)\n",
    "        \n",
    "        # Get action probabilities using batched MCTS\n",
    "        #policies = get_action_probs_batch(states, mctss, simulations)\n",
    "        #policies = np.nan_to_num([policy/np.sum(policy) for policy in policies], nan=1/81)\n",
    "        \n",
    "        # Sample actions using the policies\n",
    "        #action_indices = np.array([np.random.choice(81, p=policy) for policy in policies])\n",
    "        #actions = [index_to_action(int(idx)) for idx in action_indices]\n",
    "        \n",
    "        # Save state, player, policy in memory\n",
    "        for i in range(num_games):\n",
    "            if active_games[i]:\n",
    "                policy = policies[i] / np.sum(policies[i])  # Normalize policy\n",
    "                #games_mem[i].append([board_to_array(states[i]), states[i].fill_num, policies[i], None])\n",
    "                #games_mem[i].append([board_to_array(states[i]), states[i].fill_num, policy, None])\n",
    "                games_mem[i].append([states[i], states[i].fill_num, policy, None])\n",
    "                action_index = np.random.choice(len(policy), p=policy)\n",
    "                action = index_to_action(action_index)\n",
    "                #states[i] = change_state(states[i], actions[i])\n",
    "                states[i] = change_state(states[i], action)\n",
    "    \n",
    "    return games_mem\n",
    "\n",
    "def test_playgame():\n",
    "    mcts = MCTS(model, cpuct)\n",
    "    start_time = time.time()\n",
    "    p=[]\n",
    "    for _ in range(parallel_games):\n",
    "        p.append(playgame(mcts))\n",
    "    end_time = time.time()\n",
    "    print(end_time-start_time)\n",
    "    \n",
    "def test_playgames():\n",
    "    mctss = [MCTS(model, cpuct) for _ in range(parallel_games)]\n",
    "    start_time=time.time()\n",
    "    ps=playgames(mctss,parallel_games)\n",
    "    end_time=time.time()\n",
    "    print(end_time-start_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, games_mem, epochs=4, batch_size=32, lr=0.0001):\n",
    "    \"\"\"\n",
    "    Train the neural network using the game memory (states, policies, and results).\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        games_mem: A list of list of game memories, each containing [state_array, current_player, policy, result].\n",
    "    \n",
    "    Returns:\n",
    "        model: The trained model.\n",
    "    \"\"\"\n",
    "    print(\"Training Network\")\n",
    "    sumlen =sum([len(game_mem) for game_mem in games_mem])\n",
    "    print(\"Length of game_mem:\", sumlen)\n",
    "    \n",
    "    states = []\n",
    "    policies = []\n",
    "    values = []\n",
    "\n",
    "    # Prepare the training data from game memory\n",
    "    for game_mem in games_mem:\n",
    "        for mem in game_mem:\n",
    "            # Extract state, policy, and result (value)\n",
    "            states.append(mem[0])  # mem[0] is the board state\n",
    "            policies.append(mem[2])  # mem[2] is the action policy\n",
    "            values.append(mem[3])   # mem[3] is the game result (1, 0, or -1)\n",
    "        \n",
    "    states = torch.stack([state_to_tensor(state) for state in states])\n",
    "    policies = torch.tensor(policies, dtype=torch.float32).to(device)\n",
    "    values = torch.tensor(values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    #state = np.array(state)  # Convert the list of states to a numpy array\n",
    "    #policy = np.array(policy)  # Convert the list of policies to a numpy array\n",
    "    #value = np.array(value)  # Convert the list of values to a numpy array\n",
    "        \n",
    "    #states = torch.tensor(np.array(state), dtype=torch.float32).to(device)\n",
    "    #policies = torch.tensor(np.array(policy), dtype=torch.float32).to(device)\n",
    "    #values = torch.tensor(np.array(value), dtype=torch.float32).to(device)\n",
    "\n",
    "    # Train the neural network on the collected data\n",
    "    #states = states.unsqueeze(0)  # Add channel dimension, resulting shape: [batch_size, 1, 9, 9]\n",
    "    optimizer = get_optimizer(model,lr)\n",
    "    #Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Shuffle the game memory for each epoch\n",
    "        indices = torch.randperm(sumlen)\n",
    "        states = states[indices]\n",
    "        policies = policies[indices]\n",
    "        values = values[indices]\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, sumlen, batch_size):\n",
    "            batch_states = states[i:i + batch_size]\n",
    "            batch_policies = policies[i:i + batch_size]\n",
    "            batch_values = values[i:i + batch_size]\n",
    "\n",
    "            # Zero the gradients\n",
    "            #optimizer.zero_grad()\n",
    "            for param in model.parameters:\n",
    "                param.grad=None\n",
    "            \n",
    "            total_loss = 0\n",
    "            \n",
    "            for j in range(batch_states.size(0)):\n",
    "                # Get the single state for the current iteration\n",
    "                #state_single = batch_states[j].unsqueeze(0).unsqueeze(1)  # Shape: [1, 1, 9, 9]\n",
    "                #policy_single = batch_policies[j].unsqueeze(0)  # Shape: [1, 81]\n",
    "                #value_single = batch_values[j]  # Shape: [1]\n",
    "                state_single = batch_states[j]\n",
    "                policy_single = batch_policies[j].unsqueeze(0)\n",
    "                value_single = batch_values[j].unsqueeze(0).unsqueeze(1)\n",
    "                \n",
    "                # Forward pass for the single state\n",
    "                predicted_policy, predicted_value = model(state_single)\n",
    "                \n",
    "                #print(predicted_policy.shape, policy_single.shape)\n",
    "                #print(predicted_value.shape, value_single.shape)\n",
    "\n",
    "                # Compute loss for the single state\n",
    "                policy_loss = compute_policy_loss(predicted_policy, policy_single)\n",
    "                value_loss = compute_value_loss(predicted_value, value_single)\n",
    "\n",
    "                # Accumulate the total loss\n",
    "                total_loss += (policy_loss + value_loss)\n",
    "\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Optimizer step (update weights)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pit(old_model, new_model):\n",
    "    \"\"\"\n",
    "    Pits the old neural network (old_model) against the new one (new_model).\n",
    "    The new network must win at least 52% of games to be accepted.\n",
    "\n",
    "    Args:\n",
    "        old_model: The old neural network.\n",
    "        new_model: The newly trained neural network.\n",
    "\n",
    "    Returns:\n",
    "        True if the new network is better (win rate >= 52%), otherwise False.\n",
    "    \"\"\"\n",
    "    print(\"Pitting networks...\")\n",
    "\n",
    "    old_model_wins = 0\n",
    "    new_model_wins = 0\n",
    "    total_games = n_pit_network\n",
    "    nets = [None, old_model, new_model]\n",
    "\n",
    "    for game in range(total_games):\n",
    "        state = State()  # Start with an empty board\n",
    "        mover = 1 if game%2==0 else -1\n",
    "\n",
    "        while True:\n",
    "            # Select which network plays\n",
    "            net = nets[mover]\n",
    "            \n",
    "            # Get action probabilities from the network\n",
    "            pi, _ = net(state_to_tensor(state))\n",
    "\n",
    "            # Mask invalid actions\n",
    "            valid_actions = get_all_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                break  # No valid actions left, game is a tie\n",
    "   \n",
    "            valid_mask = np.zeros(81)\n",
    "            action_idxs = [action_to_index(a) for a in valid_actions]\n",
    "            valid_mask[action_idxs] = 1\n",
    "            pi = pi.detach().cpu().numpy().reshape(81) * valid_mask  # Convert to numpy and apply valid mask\n",
    "            pi = pi / np.sum(pi)  # Normalize the policy to sum to 1\n",
    "            \n",
    "            #the problem here is that in inference mode, you should take best move\n",
    "            #however, the normal alphazero method is to train the model to predict pi, v to improve MCTS\n",
    "            #then during gameplay to use the best move from the improved MCTS\n",
    "            #what is done here is instead to do MCTS and then use these to train the model to predict pi, v\n",
    "            #and to use the best move from the predicted policy\n",
    "            #since we need to train the model to predict v for use in minimax\n",
    "            #therefore we cannot use best move here since best move will be deterministic\n",
    "            # Choose the best move\n",
    "            #action_index = int(np.argmax(policy))\n",
    "            #action = torch.argmax(policy).item()\n",
    "            action_index = np.random.choice(len(pi), p=pi) #or add dirichlet noise and take max\n",
    "            \n",
    "            # Execute the move\n",
    "            #print(state, action_index)\n",
    "            state = change_state(state, index_to_action(action_index))\n",
    "            \n",
    "            if is_terminal(state):\n",
    "                #print(state)\n",
    "                if terminal_utility(state) == 1.0:  # Player 1 wins\n",
    "                    old_model_wins += 1 if game%2==0 else 0\n",
    "                    new_model_wins += 1 if game%2==1 else 0\n",
    "                if terminal_utility(state) == 0.0:  # Player 2 wins\n",
    "                    old_model_wins += 1 if game%2==1 else 0\n",
    "                    new_model_wins += 1 if game%2==0 else 0\n",
    "                break  # Game over\n",
    "\n",
    "            # Switch players            \n",
    "            mover *= -1\n",
    "\n",
    "    total_wins = old_model_wins + new_model_wins\n",
    "    \n",
    "    if total_wins == 0:\n",
    "        print(\"All games ended in a tie.\")\n",
    "        now = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f'tictactoeTie{now}.pth'\n",
    "        model_path = os.path.join(save_model_path, filename)\n",
    "        torch.save(old_model.state_dict(), model_path)\n",
    "        return False\n",
    "\n",
    "    old_model_win_percent = old_model_wins / total_wins\n",
    "    new_model_win_percent = new_model_wins / total_wins\n",
    "    print(f\"Old NN win rate: {old_model_win_percent:.2%} ({old_model_wins} / {total_wins} wins)\")\n",
    "    print(f\"New NN win rate: {new_model_win_percent:.2%} ({new_model_wins} / {total_wins} wins)\")\n",
    "\n",
    "    if new_model_win_percent >= threshold:\n",
    "        print(\"The new network is better!\")\n",
    "        now = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f'tictactoeWin{now}.pth'\n",
    "        model_path = os.path.join(save_model_path, filename)\n",
    "        torch.save(new_model.state_dict(), model_path)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The new network lost.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = UTTT()\n",
    "#model.to(device)\n",
    "\n",
    "def train(episodes=100, num_games=8, simulations=200):\n",
    "    \"\"\"\n",
    "    Trains the neural network using self-play, MCTS, and reinforcement learning.\n",
    "    Saves the best model based on self-play evaluations.\n",
    "    \"\"\"\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    if os.path.isfile('temp.pth'):\n",
    "        print(\"Resuming training from last checkpoint\")\n",
    "        model.load_state_dict(torch.load('temp.pth', map_location=device))\n",
    "    else:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    games_mem = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        print(f\"Episode {episode + 1}/{episodes}\")\n",
    "        start_time = time.time()\n",
    "        #print(start_time)\n",
    "        \n",
    "        # Save the current model as a temporary model\n",
    "        torch.save(model.state_dict(), 'temp.pth')\n",
    "        \n",
    "        # Load the saved model\n",
    "        #old_model = nn.__class__()  # Instantiate a new model of the same class\n",
    "        old_model = UTTT()\n",
    "        old_model.load_state_dict(torch.load('temp.pth'))\n",
    "        old_model.to(device)\n",
    "        \n",
    "        mctss = [MCTS(model, cpuct) for _ in range(parallel_games)]\n",
    "\n",
    "        # Self-play to generate training data\n",
    "        #for _ in range(playgames_before_training):\n",
    "            #games_mem += playgame()\n",
    "        games_mem += playgames(mctss, num_games=num_games, simulations=simulations)\n",
    "\n",
    "        # Train the network with collected data\n",
    "        train_model(model, games_mem)\n",
    "\n",
    "        # Clear memory after training\n",
    "        games_mem = []\n",
    "\n",
    "        # Compare old vs. new network through self-play\n",
    "        if pit(old_model, model):\n",
    "            del old_model\n",
    "        else:\n",
    "            # If new NN is worse, revert back to old NN\n",
    "            #model.load_state_dict(torch.load('temp.pth'))\n",
    "            model.load_state_dict(old_model.state_dict())\n",
    "            del old_model\n",
    "        \n",
    "        end_time = time.time()\n",
    "        #print(end_time)\n",
    "        \n",
    "        #print((end_time-start_time)/60)\n",
    "        print(f\"Episode {episode + 1} took {(end_time - start_time) / 60} minutes.\")\n",
    "\n",
    "    # Save the final trained model\n",
    "    now = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f'tictactoe_MCTS{episodes}_{now}.pth'\n",
    "    model_path = os.path.join(save_model_path, filename)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(model.state_dict(), 'temp.pth')\n",
    "    print(f\"Training complete. Model saved as {filename}\")\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    print(f\"Training {episodes} took {(total_end_time - total_start_time) / 3600} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = UTTT()\n",
    "inference_model.load_state_dict(torch.load('temp.pth'))\n",
    "inference_model.to(device)\n",
    "\n",
    "def get_move(state, model):\n",
    "    valid_actions = get_all_valid_actions(state)\n",
    "    ev,best_action = float('inf'),None\n",
    "    for action in valid_actions:\n",
    "        next_state=change_state(state,action)\n",
    "        _,v=model(state_to_tensor(next_state)) #this gives v from next player POV due to flip #v measures how good for the CURRENT PLAYER\n",
    "        if v<ev: ev,best_action = v,action\n",
    "    return best_action\n",
    "\n",
    "class StudentAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates your agent.\n",
    "        \"\"\"\n",
    "\n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        \"\"\"Returns a valid action to be played on the board.\n",
    "        Assuming that you are filling in the board with number 1.\n",
    "\n",
    "        Parameters\n",
    "        ---------------\n",
    "        state: The board to make a move on.\n",
    "        \"\"\"\n",
    "        #return best_move(state, 1, inference_model)\n",
    "        return get_move(state, inference_model)\n",
    "\n",
    "# Use this cell to test your agent in two full games against a random agent.\n",
    "# The random agent will choose actions randomly among the valid actions.\n",
    "\n",
    "class RandomStudentAgent(StudentAgent):\n",
    "    def choose_action(self, state: State) -> Action:\n",
    "        # If you're using an existing Player 1 agent, you may need to invert the state\n",
    "        # to have it play as Player 2. Uncomment the next line to invert the state.\n",
    "        # state = state.invert()\n",
    "\n",
    "        # Choose a random valid action from the current game state\n",
    "        return state.get_random_valid_action()\n",
    "\n",
    "def run(your_agent: StudentAgent, opponent_agent: StudentAgent, start_num: int):\n",
    "    your_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    opponent_agent_stats = {\"timeout_count\": 0, \"invalid_count\": 0}\n",
    "    turn_count = 0\n",
    "    \n",
    "    state = State(fill_num=start_num)\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "        #print(state)\n",
    "        turn_count += 1\n",
    "\n",
    "        agent_name = \"your_agent\" if state.fill_num == 1 else \"opponent_agent\"\n",
    "        agent = your_agent if state.fill_num == 1 else opponent_agent\n",
    "        stats = your_agent_stats if state.fill_num == 1 else opponent_agent_stats\n",
    "\n",
    "        start_time = time.time()\n",
    "        action = agent.choose_action(state.clone())\n",
    "        end_time = time.time()\n",
    "        \n",
    "        random_action = state.get_random_valid_action()\n",
    "        if end_time - start_time > 3:\n",
    "            print(f\"{agent_name} timed out!\")\n",
    "            stats[\"timeout_count\"] += 1\n",
    "            action = random_action\n",
    "        if not state.is_valid_action(action):\n",
    "            print(f\"{agent_name} made an invalid action!\")\n",
    "            stats[\"invalid_count\"] += 1\n",
    "            action = random_action\n",
    "                \n",
    "        #print(action)\n",
    "        print(inference_model(state_to_tensor(state)))\n",
    "        state = state.change_state(action)\n",
    "\n",
    "    print(f\"== {your_agent.__class__.__name__} (1) vs {opponent_agent.__class__.__name__} (2) - First Player: {start_num} ==\")\n",
    "        \n",
    "    if state.terminal_utility() == 1:\n",
    "        print(\"You win!\")\n",
    "    elif state.terminal_utility() == 0:\n",
    "        print(\"You lose!\")\n",
    "    else:\n",
    "        print(\"Draw\")\n",
    "\n",
    "    for agent_name, stats in [(\"your_agent\", your_agent_stats), (\"opponent_agent\", opponent_agent_stats)]:\n",
    "        print(f\"{agent_name} statistics:\")\n",
    "        print(f\"Timeout count: {stats['timeout_count']}\")\n",
    "        print(f\"Invalid count: {stats['invalid_count']}\")\n",
    "        \n",
    "    print(f\"Turn count: {turn_count}\\n\")\n",
    "    #print(state)\n",
    "\n",
    "your_agent = lambda: StudentAgent()\n",
    "opponent_agent = lambda: RandomStudentAgent()\n",
    "\n",
    "run(your_agent(), opponent_agent(), 1)\n",
    "run(your_agent(), opponent_agent(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "352594b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180162\n",
      "169713\n",
      "['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'pi.weight', 'pi.bias', 'v.weight', 'v.bias']\n",
      "['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'v.weight', 'v.bias']\n",
      "_UTTT(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1296, out_features=128, bias=True)\n",
      "  (v): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=10, threshold=200000)\n",
    "\n",
    "model1 = UTTT(False, False)\n",
    "model2 = _UTTT(False, False)\n",
    "\n",
    "print(sum(p.numel() for p in model1.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in model2.parameters() if p.requires_grad))\n",
    "\n",
    "print([k for k,v in model1.state_dict().items()])\n",
    "\n",
    "print([k for k,v in model2.state_dict().items()])\n",
    "\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e7481e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_weights.txt\", \"w\") as f:\n",
    "    print(model2.state_dict(),file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0375c570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs=model2.state_dict()\n",
    "model3=_UTTT(False,False)\n",
    "model3.load_state_dict(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd13c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
